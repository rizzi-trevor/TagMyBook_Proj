---
title: "TagMyBook"
author: "Trevor Rizzi"
date: "2022-11-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The TagMyBook dataset is filled with different books and their attributes. This data was found on Kaggle, <https://www.kaggle.com/code/deblina00/book-genre-prediction/data>, created by Deblina Ghosh and has a usability index of 10. This dataset is an expansion of the dataset I had chosen in my initial project data memo. It has many more entries than the original, which I hope will strengthen my models. The attributes of interest are genre and summary I want to be able to predict the genre of a book based on any input synopsis. I will primarily be looking at string/text variables, and as such will need to use natural language processing. There are a few tools in R that can help with this such as: Tidytext, Textrecipes, and more. The synopsis may contain a few key words that are indicative of the genre. This is a classification approach as it concerns qualitative data. This will be a predictive model for genre prediction.

# Why I chose this data

As humans, when picking books what do we do? Read the synopsis on the back. This gives us a general idea of the genre and happenings of the book and helps us determine if we want to read it or not. I believe that training a computer to predict the genres of a book is the first step towards mimicking what we do. It could also be useful for categorizing books with unspecified genres. Most of all, I am extrememly interested in text processing and found this to be a great introduction.

```{r}
library(tidyverse)
library(lubridate)
library(tidymodels)
library(patchwork)
library(janitor)
library(tidytext)
library(parsnip)
library(textrecipes)
library(discrim) 
library(themis)
library(hardhat)
library(stopwords) ## Removing stop words
library(irlba)
library(kknn)
library(wordcloud)
library(StrTrunc)
library(keras)

# Deprecated data set, had much fewer entries
#tagMyBook <- read.csv("/Users/trevorrizz/Documents/Pstat131-FinalProj/data.csv")

tagMyBook <- read.csv("/Users/trevorrizz/Documents/TagMyBook_Proj/dataTWO.csv")

tagMyBook <- tagMyBook %>%
  clean_names() %>% 
  mutate_at(vars(genre), as.factor)
set.seed(9388)


```

## Data Cleaning

```{r}
head(tagMyBook)
```

We now have 4657 book entries and 4 columns. This data has no missing values and is already extremely clean, so we just need to get it into the format we want.

```{r}
length(unique(tagMyBook$genre)) ## WE have 10 different genres
length(unique(tagMyBook$title))  # we have 4296 unique books

table(tagMyBook$genre) ## Shows how many of each genre we have.
str(tagMyBook)
```

We have 10 different genres, and 4296 unique books. It is clear that the data is very unbalanced, thriller is 24% of the data while sports, psychology, and travel combined constitute about 7% of the data. This is an indication that I might want to consider downsampling later on when completing the models.

```{r}

tagMyBook_EDA <- tagMyBook %>%
  unnest_tokens(word, summary) %>%
  filter(!grepl('[0-9]', word)) %>% ## gets rid of numbers
  filter(!grepl("[a1~!@#$%^&*(){}_+:\"<>?,./;'[]-=]" , word))

```

## Tokenization

Before we perform any analysis on the data, we must tokenize the summary column. This is a fundamental step to natural language processing. It separates each piece of text into tokens. In this case we will be tokenizing the summary into individual words, using space as a delimiter. I also chose to remove any tokens that are numbers since they seem irrelevant when considering a book's genre.

```{r}

tagMyBook_EDA %>%
  count(word, sort = T) # checking word count before removing STOP WORDS

tagMyBook_EDA %>%
  count(word == "the")

tagMyBook_EDA <- tagMyBook_EDA %>%
  filter(!(word %in% stopwords(source = "snowball")))

```

## Stop Words

Stop words are the most common words in a language. These are typically articles, prepositions, pronouns, conjuctions, etc. They do not add much information to the text so we are going to remove them. "The" is a great example of a stop word that is used 116615 times in the data but is not helpful at all in predicting a genre since it gives off no connotation. We went from 1,679,409 tokens to 920,979 after removing stop words.

# Data Analysis

```{r}
ggplot(tagMyBook_EDA, aes(genre)) + geom_bar() + 
  labs(
    title = "Genre Matched with Word count",
    x = "Genre",
    y = "Count of Individual Words"
  
)

```

This graph shows the total word count for each genre. It is no surprise that the minority genres have much lower word counts. This is a huge concern as the model is likely going to struggle with predicting those genres.

```{r}
tagMyBook_EDA %>% 
  count(word, sort = TRUE) %>%
  filter(n > 2000) %>% 
  mutate(word = reorder(word, n)) %>% 
    ggplot(aes(word, n)) + 
    geom_col() +
    coord_flip() +
    labs(x = "Word \n", y = "\n Count ", title = "Frequent Words In Dataset") +
    geom_text(aes(label = n), hjust = 1.2, colour = "white", fontface = "bold") +
    theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="darkblue", size = 12),
        axis.title.y = element_text(face="bold", colour="darkblue", size = 12))
```

This bar graph shows the top 12 words in the synopses of all of the books in the dataset. It is interesting to note that words such as "one" may not seem like it would have significant information pertinent to a books genre, but it still carries an important connotation that could effect the genre. This leads me into the next part of exploration, sentiment analysis.

## Sentiment Analysis

Sentiment analysis is a technique to determine whether data is positive or negative. This is a subconscious mechanism humans do when parsing through data to understand their meaning, so we should see if the connotation of words can give us some insight on the genres.

```{r}
positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

## Just lists the count of all positive sentiment words
tagMyBook_EDA %>%
  semi_join(positive) %>% 
  count(word, sort = TRUE)


## Can plot the change of sentiment among genres
bing <- get_sentiments("bing")

genresentiment <- tagMyBook_EDA %>%
  inner_join(bing) %>%
  count(genre, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(genresentiment, aes( genre, sentiment)) +
  geom_bar(stat = "identity", show.legend = FALSE)



```

This is a graph of the sentiment of the words in each genre. Clearly, it is overwhelmingly negative. This, however, makes quite a bit of sense with genres such as crime, horror, thriller, and even history. The genre titles themselves have negative connotations, so it is not unreasonable to expect most of their synopses to have negative sounding words as well.

```{r}
bing_sentiment_count <- tagMyBook_EDA %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)

bing_sentiment_count %>%
  filter(n > 500) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")


```

This graph shows the top words in the data, and illustrates their sentiment. It has some good examples to understand what kind of words are labelled as negative and positive.

```{r}

## Lets look at the sentiment of entire synopsis now yeah?

bingnegative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

bingpositive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")


wordcounts <- tagMyBook_EDA %>%
  group_by(title) %>%
  summarize(words = n())

negative_synopses<-tagMyBook_EDA %>%
  semi_join(bingnegative) %>%
  group_by(title) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("title")) %>%
  mutate(ratio = negativewords/words) 

positive_synopses<-tagMyBook_EDA %>%
  semi_join(bingpositive) %>%
  group_by(title) %>%
  summarize(positivewords = n()) %>%
  left_join(wordcounts, by = "title") %>%
  mutate(ratio = positivewords/words)
  

positive_synopses %>%
  filter(ratio > .20) %>%
  ggplot(aes(ratio * 100, str_trunc(title, 15))) + labs(x = "Positivity Percentage", y = "Title", title = "Positive Books") + geom_col() +

negative_synopses %>%
  filter(ratio > .28) %>%
  ggplot(aes(ratio * 100, str_trunc(title, 15))) + labs(x = "Negativity Percentage", y = "Title", title = "Negative Books") + geom_col()

```

This graph shows us the most negative and the most positive books.

```{r}

# a plot of the frequency of top words in each genre sounds

tagMyBook_EDA %>%
  count(word, genre) %>%
  filter(n >= 10) %>%  ## SHOULD PROBABLY INCLUDE THIS EVERYWHERE !
  bind_tf_idf(word, genre, n) %>% 
  group_by(genre) %>% 
  top_n(tf_idf,n = 5) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder_within(word, tf_idf, genre), y = tf_idf, fill = genre)) + geom_col() + scale_x_reordered() + coord_flip() + facet_wrap(~genre, scales = "free") + theme(legend.position = "none")


```

This graph shows the most common words per each genre. It is interesting to note that there are a lot of proper nouns in some of these genres. I could create a list of exclusions with them, but I am convinced that the proper nouns do have significant meaning to a book's genre.

```{r, fig.height = 15, fig.width = 15}
sparse_df <- tagMyBook %>% 
  select(genre, summary) %>% 
  unnest_tokens("word", "summary") %>% 
  count(genre, word) %>% 
  anti_join(stop_words) %>% 
  filter(n>=20) %>%
  group_by(genre) %>%
  top_n(n, n= 25) %>%
  ungroup() %>%
  cast_sparse(row = genre, column = word, value = n)

pca_text <- prcomp_irlba(sparse_df, n=4, scale = TRUE)

pca_text$center %>% 
  tidy() %>% 
  select(names) %>% 
  cbind(pca_text$rotation) %>% 
  ggplot(aes(x = PC1, y = PC2, label = names)) + geom_point() + geom_text()
```

In this we can see multiple defined clusters such as {"battle","return","king"} and {"murder", "found", "police"}

# Data Splitting

```{r}
tagMyBook_split <- tagMyBook %>%
  initial_split(prop = .80, strata = genre)

tagMyBook_test <- testing(tagMyBook_split)

tagMyBook_train <- training(tagMyBook_split)

crossValidation <- vfold_cv(tagMyBook_train, v = 5, strata = genre, repeats = 5)

```


# Model Building

## Naive Bayes
```{r}
#############NAIVE BAYES
complaints_rec <- recipe(genre~summary, data = tagMyBook_train) %>%
  step_tokenize(summary) %>%
  step_tokenfilter(summary, max_tokens = 1e3) %>%
  step_tfidf(summary)

complaint_wf <- workflow() %>%
  add_recipe(complaints_rec)


nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_fit <- complaint_wf %>%
  add_model(nb_spec) %>%
  fit(data = tagMyBook_train)



nb_wf <- workflow() %>%
  add_recipe(complaints_rec) %>%
  add_model(nb_spec)



nb_rs <- fit_resamples(
  nb_wf,
  crossValidation,
  control = control_resamples(save_pred = TRUE)
)


nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)


## SO WE NEED TO DECIDE IF WE WANT "ACCURACY"" OR "ROC_AUC" FOR OUR METRICS....
nb_best_acc <- nb_rs %>%
  show_best("accuracy")

nb_best_acc

best_nb <- select_best(nb_rs, metric = "roc_auc")
best_nb

nb_final <- finalize_workflow(nb_wf, best_nb)

nb_final_fit <- fit(nb_final, data = tagMyBook_train)

nb_best_fit <- nb_final_fit %>%
  pull_workflow_fit()


autoplot(nb_best_fit) ## NIT WORKING
###------


nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = genre, estimate = c(.pred_crime, .pred_fantasy, .pred_history, .pred_horror, .pred_psychology, .pred_romance, .pred_science, .pred_sports, .pred_thriller, .pred_travel)) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for Book Genre",
    subtitle = "Each resample repeat is shown in a different color"
  )
#####################################

```

Clearly the model doesnt even make predictions for the minority genres, and we should probably do something about that or talk about it more and continue . The other genres fit pretty alright, I suppose psych romance sports and travel just did not have enough data points.

Maybe we can do some downsampling !!!!!!!


## Lasso
```{r}

#################YEWWWW LASSO BABAAAY


lasso_rec <-
  recipe(genre ~ summary,
         data = tagMyBook_train) %>%
  step_tokenize(summary) %>%
  step_tokenfilter(summary, max_tokens = 1e3) %>%
  step_tfidf(summary) %>%
  step_downsample(genre)


lasso_spec <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")


lasso_spec


sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")

lasso_wf <- workflow() %>%
  add_recipe(lasso_rec, blueprint = sparse_bp) %>%
  add_model(lasso_spec)

lasso_wf

param_grid <- grid_regular(penalty(range = c(-5, 0)), levels = 20)

lasso_rs <- tune_grid(
  lasso_wf,
  crossValidation,
  grid = param_grid,
  control = control_resamples(save_pred = TRUE)
)

lasso_best_acc <- lasso_rs %>%
  show_best("accuracy")

lasso_best_acc

lasso_rs %>%
  collect_predictions() %>%
  filter(penalty == lasso_best_acc$penalty) %>%
  conf_mat(genre, .pred_class) %>%
  autoplot(type = "heatmap") +
  scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
  scale_x_discrete(labels = function(x) str_wrap(x, 20))

## THIS ISNT DOING JUST THE TOP ONE HUH
lasso_metrics <- collect_metrics(lasso_rs)
lasso_predictions <- collect_predictions(lasso_rs)


lasso_predictions %>%
  group_by(id) %>%
  roc_curve(truth = genre, estimate = c(.pred_crime, .pred_fantasy, .pred_history, .pred_horror, .pred_psychology, .pred_romance, .pred_science, .pred_sports, .pred_thriller, .pred_travel)) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for Book Genre",
    subtitle = "Each resample repeat is shown in a different color"
  )

```


## Random Forest
```{r}


rf_rec <-
  recipe(genre ~ summary,
         data = tagMyBook_train) %>%
  step_tokenize(summary) %>%
  step_tokenfilter(summary, max_tokens = 1e3) %>%
  step_tfidf(summary) %>%
  step_downsample(genre)



rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_rec)

param_grid <- grid_regular(mtry(range = c(1, 1)),trees(range = c(3, 6)), min_n(range = c(3, 6)),  levels = 8)

rf_rs <- tune_grid(
  rf_wf, 
  resamples = crossValidation, 
  grid = param_grid,
  metrics = metric_set(roc_auc),
  control = control_resamples(save_pred = TRUE)
)


autoplot(rf_rs)

rf_best_acc <- rf_rs %>%
  show_best("roc_auc")

rf_best_acc

rf_metrics <- collect_metrics(rf_rs)
rf_predictions <- collect_predictions(rf_rs)
rf_predictions %>%
  group_by(id) %>%
  roc_curve(truth = genre, estimate = c(.pred_crime, .pred_fantasy, .pred_history, .pred_horror, .pred_psychology, .pred_romance, .pred_science, .pred_sports, .pred_thriller, .pred_travel)) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for Book Genre",
    subtitle = "Each resample repeat is shown in a different color"
  )

```

The random forest model did not perform as well as the lasso model, but did beat the naive bayes model as it actually managed to predict the minority genres with atleast a little accuracy

We should use some synopses from books in the dataset and some not in it to run the prediction model and see what we get. Maybe 1 of each.

## Haven't chosen yet cus the knn not work
```{r}
knn_rec <-
  recipe(genre ~ summary,
         data = tagMyBook_train) %>%
  step_tokenize(summary) %>%
  step_tokenfilter(summary, max_tokens = 1e3) %>%
  step_tfidf(summary) %>%
  step_downsample(genre)


knn_model <- 
  nearest_neighbor(
    neighbors = tune(),
    mode = "regression") %>% 
  set_engine("kknn")

knn_workflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(knn_rec)


knn_params <- parameters(knn_model)


# define grid
knn_grid <- grid_regular(knn_params, levels = 2)

knn_rs <- knn_workflow %>% 
  tune_grid(
    # what will it fit the workflow to
    resamples = crossValidation, 
    # how does it complete the models in those workflows
            grid = knn_grid)


autoplot(knn_rs)

knn_best_acc <- knn_rs %>%
  show_best("roc_auc")

knn_best_acc

knn_metrics <- collect_metrics(knn_rs)
knn_predictions <- collect_predictions(knn_rs)
knn_predictions %>%
  group_by(id) %>%
  roc_curve(truth = genre, estimate = c(.pred_crime, .pred_fantasy, .pred_history, .pred_horror, .pred_psychology, .pred_romance, .pred_science, .pred_sports, .pred_thriller, .pred_travel)) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "ROC curve for Book Genre",
    subtitle = "Each resample repeat is shown in a different color"
  )


```
```{r}
nnet_rec <-
  recipe(genre ~ summary,
         data = tagMyBook_train) %>%
  step_tokenize(summary) %>%
  step_tokenfilter(summary, max_tokens = 1e3) %>%
  step_tfidf(summary) %>%
  step_downsample(genre)


  #fit(genre ~ summary, data = bake(biv_rec, new_data = NULL))


nnet_spec <- mlp(epochs = 100, hidden_units = 5, dropout = .1) %>%
  set_mode("classification") %>%
  set_engine("keras", verbose = 0)


nnet_wf <- workflow() %>%
  add_model(nnet_spec) %>%
  add_recipe(nnet_rec)


param_grid <- grid_regular(tune(), levels = 20)

nnet_rs <- tune_grid(
  nnet_wf,
  crossValidation,
  grid = param_grid,
  control = control_resamples(save_pred = TRUE)
)

packages.install("tensorflow")
```

